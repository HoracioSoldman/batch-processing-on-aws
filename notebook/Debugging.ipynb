{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c90b0197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hrc/Documents/de-aws/data-venv/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/02 01:25:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "        .master('local[*]') \\\n",
    "        .appName('journey-and-stations-data-transformer') \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", os.environ.get('AWS_ACCESS_KEY'))\\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", os.environ.get('AWS_SECRET_ACCESS_KEY'))\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba2dc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/02 01:25:14 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# finally, save journey data into parquet files in s3\n",
    "df_j= spark.read.csv('s3a://hrc-de-data/raw/cycling-extras/journey.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34f20e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3881bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "df_j= df_j.withColumnRenamed('Rental Id', 'rental_id')\\\n",
    ".withColumnRenamed('Bike Id', 'bike_id')\\\n",
    ".withColumnRenamed('Start Date', 'start_date')\\\n",
    ".withColumnRenamed('End Date', 'end_date')\\\n",
    ".withColumnRenamed('StartStation Id', 'start_station')\\\n",
    ".withColumnRenamed('EndStation Id', 'end_station')\n",
    "\n",
    "# convert data types\n",
    "df_j= df_j.withColumn('start_date', to_timestamp(col('start_date'), 'dd/MM/yyy HH:mm'))\n",
    "\n",
    "df_j= df_j.withColumn('end_date',  to_timestamp(col('end_date'), 'dd/MM/yyy HH:mm'))\n",
    "\n",
    "# add weather_date column\n",
    "df_j= df_j.withColumn('weather_date', to_date(col(\"start_date\"), 'dd/MM/yyy HH:mm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb53c2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-------------------+-----------+--------------------+-------------------+-------------+--------------------+------------+\n",
      "|rental_id|Duration|bike_id|           end_date|end_station|     EndStation Name|         start_date|start_station|   StartStation Name|weather_date|\n",
      "+---------+--------+-------+-------------------+-----------+--------------------+-------------------+-------------+--------------------+------------+\n",
      "|104820582|    1620|     22|2021-01-03 15:14:00|         11|Brunswick Square,...|2021-01-03 14:47:00|          542|Salmon Lane, Lime...|  2021-01-03|\n",
      "|104816169|    1740|  10755|2021-01-03 13:26:00|        542|Salmon Lane, Lime...|2021-01-03 12:57:00|          546|New Fetter Lane, ...|  2021-01-03|\n",
      "|104757113|    1620|  18908|2020-12-30 15:00:00|        239|Warren Street Sta...|2020-12-30 14:33:00|          779|Houndsditch, Aldgate|  2020-12-30|\n",
      "|104749458|     780|  18499|2020-12-30 09:21:00|        766|Ram Street, Wands...|2020-12-30 09:08:00|          653|Simpson Street, C...|  2020-12-30|\n",
      "|104788389|    5400|  15668|2021-01-01 16:29:00|        655|Crabtree Lane, Fu...|2021-01-01 14:59:00|          655|Crabtree Lane, Fu...|  2021-01-01|\n",
      "+---------+--------+-------+-------------------+-----------+--------------------+-------------------+-------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- rental_id: integer (nullable = true)\n",
      " |-- Duration: integer (nullable = true)\n",
      " |-- bike_id: integer (nullable = true)\n",
      " |-- end_date: timestamp (nullable = true)\n",
      " |-- end_station: integer (nullable = true)\n",
      " |-- EndStation Name: string (nullable = true)\n",
      " |-- start_date: timestamp (nullable = true)\n",
      " |-- start_station: integer (nullable = true)\n",
      " |-- StartStation Name: string (nullable = true)\n",
      " |-- weather_date: date (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_j.show(5)\n",
    "df_j.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a321069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop other unnecessary journey columns\n",
    "df_j= df_j.drop('StartStation Name', 'EndStation Name', 'Duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b9dc3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------------------+-----------+-------------------+-------------+------------+\n",
      "|rental_id|bike_id|           end_date|end_station|         start_date|start_station|weather_date|\n",
      "+---------+-------+-------------------+-----------+-------------------+-------------+------------+\n",
      "|104820582|     22|2021-01-03 15:14:00|         11|2021-01-03 14:47:00|          542|  2021-01-03|\n",
      "|104816169|  10755|2021-01-03 13:26:00|        542|2021-01-03 12:57:00|          546|  2021-01-03|\n",
      "|104757113|  18908|2020-12-30 15:00:00|        239|2020-12-30 14:33:00|          779|  2020-12-30|\n",
      "|104749458|  18499|2020-12-30 09:21:00|        766|2020-12-30 09:08:00|          653|  2020-12-30|\n",
      "|104788389|  15668|2021-01-01 16:29:00|        655|2021-01-01 14:59:00|          655|  2021-01-01|\n",
      "|104792584|   6695|2021-01-01 21:02:00|        682|2021-01-01 20:58:00|          655|  2021-01-01|\n",
      "|104777428|   6695|2020-12-31 18:49:00|        655|2020-12-31 17:10:00|          655|  2020-12-31|\n",
      "|104791339|   6695|2021-01-01 18:28:00|        655|2021-01-01 17:57:00|          655|  2021-01-01|\n",
      "|104762231|   8883|2020-12-30 18:57:00|        682|2020-12-30 17:16:00|          655|  2020-12-30|\n",
      "|104776884|   3424|2020-12-31 16:53:00|        395|2020-12-31 16:41:00|          662|  2020-12-31|\n",
      "+---------+-------+-------------------+-----------+-------------------+-------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_j.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b8d2fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_j.write.parquet('s3a://hrc-de-data/processed/test/journey/', mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3bedf58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- start_station: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_j.select('start_station').printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83aa91a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
