{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "763a90f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25914d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9382918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hrc/Documents/de-aws/data-venv/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/27 11:52:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master('local[*]') \\\n",
    "        .appName('journey-and-stations-data-transformer') \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", os.environ.get('AWS_ACCESS_KEY'))\\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", os.environ.get('AWS_SECRET_ACCESS_KEY'))\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30406f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/27 11:53:02 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# get journey data\n",
    "df_journey = spark.read.csv(\"s3a://hrc-de-data/raw/cycling-journey/*/*\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56579ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Rental Id=109096951, Duration=540, Bike Id=13318, End Date='15/06/2021 20:19', EndStation Id=661, EndStation Name='All Saints Church, Portobello', Start Date='15/06/2021 20:10', StartStation Id=105, StartStation Name='Westbourne Grove, Bayswater'),\n",
       " Row(Rental Id=108982015, Duration=780, Bike Id=18991, End Date='13/06/2021 13:03', EndStation Id=312, EndStation Name=\"Grove End Road, St. John's Wood\", Start Date='13/06/2021 12:50', StartStation Id=106, StartStation Name='Woodstock Street, Mayfair')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_journey.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ef2dc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Rental Id: integer (nullable = true)\n",
      " |-- Duration: integer (nullable = true)\n",
      " |-- Bike Id: integer (nullable = true)\n",
      " |-- End Date: string (nullable = true)\n",
      " |-- EndStation Id: integer (nullable = true)\n",
      " |-- EndStation Name: string (nullable = true)\n",
      " |-- Start Date: string (nullable = true)\n",
      " |-- StartStation Id: integer (nullable = true)\n",
      " |-- StartStation Name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_journey.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aea050fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78224fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "df_journey= df_journey.withColumnRenamed('Rental Id', 'rental_id')\\\n",
    ".withColumnRenamed('Bike Id', 'bike_id')\\\n",
    ".withColumnRenamed('Start Date', 'start_date')\\\n",
    ".withColumnRenamed('End Date', 'end_date')\\\n",
    ".withColumnRenamed('StartStation Id', 'start_station')\\\n",
    ".withColumnRenamed('EndStation Id', 'end_station')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5054fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary column\n",
    "df_journey= df_journey.drop('Duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcd76774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the weather_id column\n",
    "df_journey= df_journey.withColumn('weather_date', F.to_date(df_journey.start_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a0d3616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rental_id: integer (nullable = true)\n",
      " |-- bike_id: integer (nullable = true)\n",
      " |-- end_date: string (nullable = true)\n",
      " |-- end_station: integer (nullable = true)\n",
      " |-- EndStation Name: string (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- start_station: integer (nullable = true)\n",
      " |-- StartStation Name: string (nullable = true)\n",
      " |-- weather_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_journey.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8da8a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "'''\n",
    "we want to complete the stations data with some additional stations \n",
    "which are not present in the original stations data but are seen in some journey.\n",
    "'''\n",
    "df_processed_stations= spark.read.parquet('s3a://hrc-de-data/processed/cycling-extras/stations/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "171f57d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(station_id=838, station_name='Fore Street Avenue, Guildhall', longitude=-0.0914017, latitude=51.518093, easting=532524.0, northing=181634.0),\n",
       " Row(station_id=839, station_name='Sea Containers, South Bank', longitude=-0.1068403, latitude=51.507974, easting=531482.0, northing=180481.0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed_stations.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52c496c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temporary table for both weather and journey\n",
    "df_journey.createOrReplaceTempView('journey')\n",
    "df_processed_stations.createOrReplaceTempView('stations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44f574f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:====================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|station_id|        station_name|\n",
      "+----------+--------------------+\n",
      "|       840|George Row, Bermo...|\n",
      "|       391|Clifford Street, ...|\n",
      "|       842|Temple Gardens, T...|\n",
      "|       844|Canada Water Stat...|\n",
      "|       845|Bermondsey Statio...|\n",
      "|       841|Tower Wharf, Berm...|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "additional_stations= spark.sql('''\n",
    "select distinct(start_station) as station_id, `StartStation Name` as station_name \n",
    "from journey \n",
    "where start_station not in (select station_id from stations)\n",
    "union\n",
    "select distinct(end_station) as station_id, `EndStation Name` as station_name \n",
    "from journey \n",
    "where end_station not in (select station_id from stations)\n",
    "''')\n",
    "additional_stations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c95d0712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add columns to the additional stations to avoid errors when merging it to the previous one (df_processed_stations)\n",
    "additional_stations= additional_stations.withColumn('longitude', F.lit(0))\\\n",
    ".withColumn('latitude', F.lit(0))\\\n",
    ".withColumn('easting', F.lit(0))\\\n",
    ".withColumn('northing', F.lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e315e5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_stations = additional_stations.withColumn('longitude', additional_stations.longitude.cast(T.DoubleType()))\\\n",
    ".withColumn('latitude', additional_stations.latitude.cast(T.DoubleType()))\\\n",
    ".withColumn('easting', additional_stations.easting.cast(T.DoubleType()))\\\n",
    ".withColumn('northing', additional_stations.northing.cast(T.DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccd5a72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- station_name: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- easting: double (nullable = true)\n",
      " |-- northing: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "additional_stations.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68e2adc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[station_id: int, station_name: string, longitude: double, latitude: double, easting: double, northing: double]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure that the additional stations does not contain duplicated entries\n",
    "additional_stations.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9361ec15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# save stations data into parquet files in s3\n",
    "additional_stations.write.parquet('s3a://hrc-de-data/processed/cycling-extras/stations/', mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2a8529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop other unnecessary journey column\n",
    "df_journey= df_journey.drop('StartStation Name', 'EndStation Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028eb71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save journey data into parquet files in s3\n",
    "df_journey.write.parquet('s3a://hrc-de-data/processed/cycling-extras/journey/', mode='append')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
