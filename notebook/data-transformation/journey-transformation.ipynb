{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deda5766",
   "metadata": {},
   "source": [
    "## Transformation for rental journey data \n",
    "This notebook is responsible for transforming journey data by performing the following tasks:\n",
    "\n",
    "    1. Renaming columns (removing spaces and lowercasing)\n",
    "\n",
    "    2. Convert data types from string to timestamps\n",
    "    \n",
    "    3. Attach weather dates\n",
    "    \n",
    "    4. Drop unnecessary columns\n",
    "    \n",
    "    5. Update extra files for dimension tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "763a90f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25914d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9382918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hrc/anaconda3/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/01 19:15:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master('local[*]') \\\n",
    "        .appName('journey-and-stations-data-transformer') \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", os.environ.get('AWS_ACCESS_KEY'))\\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", os.environ.get('AWS_SECRET_ACCESS_KEY'))\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30406f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/01 19:15:14 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# get journey data\n",
    "df_journey = spark.read.csv(\"s3a://hrc-de-data/raw/cycling-journey/*/*\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56579ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Rental Id=109096951, Duration=540, Bike Id=13318, End Date='15/06/2021 20:19', EndStation Id=661, EndStation Name='All Saints Church, Portobello', Start Date='15/06/2021 20:10', StartStation Id=105, StartStation Name='Westbourne Grove, Bayswater'),\n",
       " Row(Rental Id=108982015, Duration=780, Bike Id=18991, End Date='13/06/2021 13:03', EndStation Id=312, EndStation Name=\"Grove End Road, St. John's Wood\", Start Date='13/06/2021 12:50', StartStation Id=106, StartStation Name='Woodstock Street, Mayfair')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_journey.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ef2dc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Rental Id: integer (nullable = true)\n",
      " |-- Duration: integer (nullable = true)\n",
      " |-- Bike Id: integer (nullable = true)\n",
      " |-- End Date: string (nullable = true)\n",
      " |-- EndStation Id: integer (nullable = true)\n",
      " |-- EndStation Name: string (nullable = true)\n",
      " |-- Start Date: string (nullable = true)\n",
      " |-- StartStation Id: integer (nullable = true)\n",
      " |-- StartStation Name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_journey.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aea050fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78224fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "df_journey= df_journey.withColumnRenamed('Rental Id', 'rental_id')\\\n",
    ".withColumnRenamed('Bike Id', 'bike_id')\\\n",
    ".withColumnRenamed('Start Date', 'start_date')\\\n",
    ".withColumnRenamed('End Date', 'end_date')\\\n",
    ".withColumnRenamed('StartStation Id', 'start_station')\\\n",
    ".withColumnRenamed('EndStation Id', 'end_station')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1ad54a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data types\n",
    "df_journey= df_journey.withColumn('start_date', to_timestamp(col('start_date'), 'dd/MM/yyy HH:mm'))\n",
    "\n",
    "df_journey= df_journey.withColumn('end_date',  to_timestamp(col('end_date'), 'dd/MM/yyy HH:mm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2989d91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add weather_date column\n",
    "df_journey= df_journey.withColumn('weather_date', to_date(col(\"start_date\"), 'dd/MM/yyy HH:mm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e64f016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------------------+-----------+--------------------+-------------------+-------------+--------------------+------------+\n",
      "|rental_id|bike_id|           end_date|end_station|     EndStation Name|         start_date|start_station|   StartStation Name|weather_date|\n",
      "+---------+-------+-------------------+-----------+--------------------+-------------------+-------------+--------------------+------------+\n",
      "|109096951|  13318|2021-06-15 20:19:00|        661|All Saints Church...|2021-06-15 20:10:00|          105|Westbourne Grove,...|  2021-06-15|\n",
      "|108982015|  18991|2021-06-13 13:03:00|        312|Grove End Road, S...|2021-06-13 12:50:00|          106|Woodstock Street,...|  2021-06-13|\n",
      "|108839141|  16736|2021-06-10 15:28:00|        333|Palace Gardens Te...|2021-06-10 15:14:00|          106|Woodstock Street,...|  2021-06-10|\n",
      "|108816591|    913|2021-06-09 22:37:00|         51|Finsbury Library ...|2021-06-09 22:14:00|          123|St. John Street, ...|  2021-06-09|\n",
      "|108919084|   6682|2021-06-12 11:29:00|        732|Duke Street Hill,...|2021-06-12 11:09:00|          123|St. John Street, ...|  2021-06-12|\n",
      "+---------+-------+-------------------+-----------+--------------------+-------------------+-------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- rental_id: integer (nullable = true)\n",
      " |-- bike_id: integer (nullable = true)\n",
      " |-- end_date: timestamp (nullable = true)\n",
      " |-- end_station: integer (nullable = true)\n",
      " |-- EndStation Name: string (nullable = true)\n",
      " |-- start_date: timestamp (nullable = true)\n",
      " |-- start_station: integer (nullable = true)\n",
      " |-- StartStation Name: string (nullable = true)\n",
      " |-- weather_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_journey.show(5)\n",
    "df_journey.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71328ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5e4e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "784a2702",
   "metadata": {},
   "source": [
    "### Stations data\n",
    "We are going to update the stations data (previously saved by another process) with some additional stations that are not present in the original stations data but are seen in some journey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8da8a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# read previously saved stations data from parquet\n",
    "df_processed_stations= spark.read.parquet('s3a://hrc-de-data/processed/cycling-dimension/stations/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "171f57d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(station_id=838, station_name='Fore Street Avenue, Guildhall', longitude=-0.0914017, latitude=51.518093, easting=532524.0, northing=181634.0),\n",
       " Row(station_id=839, station_name='Sea Containers, South Bank', longitude=-0.1068403, latitude=51.507974, easting=531482.0, northing=180481.0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed_stations.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52c496c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temporary table for both stations and journey\n",
    "df_journey.createOrReplaceTempView('journey')\n",
    "df_processed_stations.createOrReplaceTempView('station')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44f574f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|station_id|        station_name|\n",
      "+----------+--------------------+\n",
      "|       840|George Row, Bermo...|\n",
      "|       391|Clifford Street, ...|\n",
      "|       842|Temple Gardens, T...|\n",
      "|       844|Canada Water Stat...|\n",
      "|       845|Bermondsey Statio...|\n",
      "|       841|Tower Wharf, Berm...|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# we keep all the stations which are not found in the temp view station table\n",
    "additional_stations= spark.sql('''\n",
    "select distinct(start_station) as station_id, `StartStation Name` as station_name \n",
    "from journey \n",
    "where start_station not in (select station_id from station)\n",
    "union\n",
    "select distinct(end_station) as station_id, `EndStation Name` as station_name \n",
    "from journey \n",
    "where end_station not in (select station_id from station)\n",
    "''')\n",
    "additional_stations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c95d0712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add columns to the additional stations to avoid errors when merging it to the previous one (df_processed_stations)\n",
    "additional_stations= additional_stations.withColumn('longitude', lit(0).cast(DoubleType()))\\\n",
    ".withColumn('latitude', lit(0).cast(DoubleType()))\\\n",
    ".withColumn('easting', lit(0).cast(DoubleType()))\\\n",
    ".withColumn('northing', lit(0).cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccd5a72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+--------+-------+--------+\n",
      "|station_id|        station_name|longitude|latitude|easting|northing|\n",
      "+----------+--------------------+---------+--------+-------+--------+\n",
      "|       840|George Row, Bermo...|      0.0|     0.0|    0.0|     0.0|\n",
      "|       391|Clifford Street, ...|      0.0|     0.0|    0.0|     0.0|\n",
      "|       842|Temple Gardens, T...|      0.0|     0.0|    0.0|     0.0|\n",
      "|       844|Canada Water Stat...|      0.0|     0.0|    0.0|     0.0|\n",
      "|       845|Bermondsey Statio...|      0.0|     0.0|    0.0|     0.0|\n",
      "+----------+--------------------+---------+--------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- station_name: string (nullable = true)\n",
      " |-- longitude: double (nullable = false)\n",
      " |-- latitude: double (nullable = false)\n",
      " |-- easting: double (nullable = false)\n",
      " |-- northing: double (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "additional_stations.show(5)\n",
    "additional_stations.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68e2adc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[station_id: int, station_name: string, longitude: double, latitude: double, easting: double, northing: double]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove duplicate values\n",
    "additional_stations.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9361ec15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# save additional stations data into parquet files in s3\n",
    "additional_stations.write.parquet('s3a://hrc-de-data/processed/cycling-dimension/stations/', mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd2a8529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop other unnecessary journey columns\n",
    "df_journey= df_journey.drop('StartStation Name', 'EndStation Name', 'Duration')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70a0ba5",
   "metadata": {},
   "source": [
    "### Datetime\n",
    "We are going to create/update datetime data from the start and end date of each journey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c813157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-----+---+----+------+------+\n",
      "|        datetime_id|year|month|day|hour|minute|second|\n",
      "+-------------------+----+-----+---+----+------+------+\n",
      "|2021-06-15 20:10:00|2021|    6| 15|  20|    10|     0|\n",
      "|2021-06-13 12:50:00|2021|    6| 13|  12|    50|     0|\n",
      "|2021-06-10 15:14:00|2021|    6| 10|  15|    14|     0|\n",
      "+-------------------+----+-----+---+----+------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------------------+----+-----+---+----+------+------+\n",
      "|        datetime_id|year|month|day|hour|minute|second|\n",
      "+-------------------+----+-----+---+----+------+------+\n",
      "|2021-06-15 20:19:00|2021|    6| 15|  20|    19|     0|\n",
      "|2021-06-13 13:03:00|2021|    6| 13|  13|     3|     0|\n",
      "|2021-06-10 15:28:00|2021|    6| 10|  15|    28|     0|\n",
      "+-------------------+----+-----+---+----+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract datetime values from the start and the end date\n",
    "df_datetime_from_start= (\n",
    "    df_journey.select(\n",
    "        col('start_date').alias('datetime_id'), \n",
    "        year(col('start_date')).alias('year'), \n",
    "        month(col('start_date')).alias('month'), \n",
    "        dayofmonth(col('start_date')).alias('day'),\n",
    "        hour(col('start_date')).alias('hour'),\n",
    "        minute(col('start_date')).alias('minute'),\n",
    "        second(col('start_date')).alias('second'),\n",
    "    )\n",
    ")\n",
    "df_datetime_from_end= (\n",
    "    df_journey.select(\n",
    "        col('end_date').alias('datetime_id'), \n",
    "        year(col('end_date')).alias('year'), \n",
    "        month(col('end_date')).alias('month'), \n",
    "        dayofmonth(col('end_date')).alias('day'),\n",
    "        hour(col('end_date')).alias('hour'),\n",
    "        minute(col('end_date')).alias('minute'),\n",
    "        second(col('end_date')).alias('second'),\n",
    "    )\n",
    ")\n",
    "\n",
    "df_datetime_from_start.show(3)\n",
    "df_datetime_from_end.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "057c1e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-----+---+----+------+------+\n",
      "|        datetime_id|year|month|day|hour|minute|second|\n",
      "+-------------------+----+-----+---+----+------+------+\n",
      "|2021-06-15 20:10:00|2021|    6| 15|  20|    10|     0|\n",
      "|2021-06-13 12:50:00|2021|    6| 13|  12|    50|     0|\n",
      "|2021-06-10 15:14:00|2021|    6| 10|  15|    14|     0|\n",
      "|2021-06-09 22:14:00|2021|    6|  9|  22|    14|     0|\n",
      "|2021-06-12 11:09:00|2021|    6| 12|  11|     9|     0|\n",
      "|2021-06-10 22:33:00|2021|    6| 10|  22|    33|     0|\n",
      "|2021-06-13 14:48:00|2021|    6| 13|  14|    48|     0|\n",
      "|2021-06-14 18:06:00|2021|    6| 14|  18|     6|     0|\n",
      "|2021-06-14 18:06:00|2021|    6| 14|  18|     6|     0|\n",
      "|2021-06-09 16:06:00|2021|    6|  9|  16|     6|     0|\n",
      "+-------------------+----+-----+---+----+------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# combine the dataframes\n",
    "df_datetime= df_datetime_from_start.union(df_datetime_from_end)\n",
    "\n",
    "# remove duplicate entries\n",
    "df_datetime.dropDuplicates()\n",
    "\n",
    "df_datetime.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38b8b2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# save datetime data into parquet files in s3\n",
    "df_datetime.write.parquet('s3a://hrc-de-data/processed/cycling-dimension/datetime/', mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "028eb71d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# finally, save journey data into parquet files in s3\n",
    "df_journey.write.parquet('s3a://hrc-de-data/processed/cycling-fact/journey/', mode='append')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
